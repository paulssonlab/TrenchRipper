{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Kymograph Code\n",
    "\n",
    "Use this notebook to figure out the kymograph hyperparameters that are most appropriate for your dataset.\n",
    "\n",
    "In the future should try to merge classes somehow...\n",
    "\n",
    "Maybe add some writing to file so I can pick up from where I left off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual, FloatSlider, IntSlider, Dropdown, IntText\n",
    "import ipywidgets as widgets\n",
    "import matplotlib\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/dask_jobqueue/config.py:12: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import trenchripper as tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be handling all steps of generating a kymograph. \n",
    "\n",
    "You will need to specify the following `args` and `kwargs` (in order):\n",
    "\n",
    "\n",
    "**Args**\n",
    "\n",
    "**input_file_prefix (string)** : File prefix for all input hdf5 files of the form \"\\[input_file_prefix\\]\\[number\\].hdf5\" This should be the default output format for the hdf5 export code, but you will need to rename files if taking input files from a different source.\n",
    "\n",
    "**all_channels (list)** : list of strings corresponding to the different image channels available in the input hdf5 file, with the channel used for segmenting trenches in the first position. NOTE: these names must match those of the input hdf5 file datasets.\n",
    "\n",
    "**fov_list (list)** : List of ints corresponding to the fovs that you wish to make kymographs of.\n",
    "\n",
    "**Kwargs**\n",
    "\n",
    "**t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in between 5 and 20 timepoints for quick processing.\n",
    "\n",
    "**t_range (tuple of ints)** : Range size to be used for subsampling input files in time.\n",
    "\n",
    "The last line will perform import and subsampling of the input hdf5 image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = '/n/scratch2/de64/2019-05-31_Bacillus_training_data_mneongreen'\n",
    "kymo = tr.interactive.kymograph_interactive(headpath,[\"channel_GFP\",\"channel_Phase\"],[1,21,36],t_subsample_step=20)\n",
    "imported_array_list = kymo.map_to_fovs(kymo.import_hdf5)\n",
    "\n",
    "# headpath = '/n/scratch2/de64/full_pipeline_test'\n",
    "# kymo = tr.interactive.kymograph_interactive(headpath,[\"channel_RFP\",\"channel_BF\"],[60],t_subsample_step=5)\n",
    "# imported_array_list = kymo.map_to_fovs(kymo.import_hdf5)\n",
    "\n",
    "# input_path_prefix = '/n/scratch2/de64/full_pipeline_test/hdf5/fov_'\n",
    "# # kymo = tr.interactive.kymograph_interactive(input_path_prefix,[\"channel_BF\",\"channel_RFP\"],[12,45],t_subsample_step=5)\n",
    "# kymo = tr.interactive.kymograph_interactive(input_path_prefix,[\"channel_RFP\"],[1,77],t_subsample_step=5,orientation_detection='phase')\n",
    "# imported_array_list = kymo.map_to_fovs(kymo.import_hdf5)\n",
    "\n",
    "# input_path_prefix = '/n/scratch2/de64/for_sylvia/timelapse/hdf5/fov_'\n",
    "# kymo = tr.interactive.kymograph_interactive(input_path_prefix,[\"channel_Phase\"],[16,70],t_subsample_step=20,t_range=(0,100))\n",
    "# imported_array_list = kymo.map_to_fovs(kymo.import_hdf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune \"trench-row\" detection hyperparameters\n",
    "\n",
    "The kymograph code begins by detecting the positions of trench rows in the image as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the y-axis by computing the qth percentile of the data along the x-axis\n",
    "2. Smooth this signal using a median kernel\n",
    "3. Use a [triangle threshold](https://imagej.net/Auto_Threshold#Triangle) to determine the trench row poisitons\n",
    "\n",
    "This method uses the following `kwargs`, which you can tune here:\n",
    "\n",
    "**y_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    "**smoothing_kernel_y_dim_0 (int)** : Median kernel size to use for step 2.\n",
    "\n",
    "**triangle_nbins (int)** : Number of bins to use in the triangle method histogram.\n",
    "\n",
    "**triangle_scaling (float)** : Scaling factor to apply to the threshold determined by the triangle method.\n",
    "\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b488ae7c74cb439b8501c67137523f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntText(value=0, description='FOV number:'), IntSlider(value=0, continuous_update=False,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "channels,shape = kymo.get_image_params(0)\n",
    "\n",
    "interact(kymo.view_image,fov_idx=IntText(value=0,description='FOV number:',disabled=False),\\\n",
    "         t=IntSlider(value=0, min=0, max=shape[-1]-1, step=1,continuous_update=False),\n",
    "        channel=Dropdown(options=channels,value=channels[0],description='Channel:',disabled=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea0afcd9f2e49b992c96011a1c1be8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=88, description='y_percentile'), IntSlider(value=17, description='smooth…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "row_detection = interactive(kymo.preview_y_precentiles, {\"manual\":True},imported_array_list=fixed(imported_array_list),y_percentile=IntSlider(value=88, min=0, max=100, step=1),\\\n",
    "         smoothing_kernel_y_dim_0=IntSlider(value=17, min=1, max=20, step=1),triangle_nbins=IntSlider(value=50, min=10, max=200, step=10),\\\n",
    "                triangle_scaling=FloatSlider(value=1.05, min=0., max=2., step=0.05));\n",
    "display(row_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate \"trench-row\" detection output\n",
    "\n",
    "After determining your desired hyperparameters, set them in the next cell and run it to produce output for later steps. **Note: The thresholding parameters do not need to be specified at this point.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_percentiles_smoothed_list = copy.copy(row_detection.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune \"trench-row\" cropping hyperparameters\n",
    "\n",
    "Next, we will use the detected rows to perform cropping of the input image in the y-dimension:\n",
    "\n",
    "1. Determine edges of trench rows based on threshold mask.\n",
    "2. Filter out rows that are too small.\n",
    "3. Perform cropping using the \"end\" of the row as reference (the end referring to the part of the trench farthest from the feeding channel).\n",
    "\n",
    "This method uses the following `kwargs`, which you can tune here:\n",
    "\n",
    "**triangle_nbins (int)** : Number of bins to use in the triangle method histogram.\n",
    "\n",
    "**triangle_scaling (float)** : Scaling factor to apply to the threshold determined by the triangle method.\n",
    "\n",
    "**y_min_edge_dist (int)** : Minimum row length necessary for detection.\n",
    "\n",
    "**padding_y (int)** : Padding to be used when cropping in the y-dimension.\n",
    "\n",
    "**trench_len_y (int)** : Length from the end of the tenches to be used when cropping in the y-dimension.\n",
    "\n",
    "**top_orientation (int)** : The orientation of the top-most row where 0 corresponds to a trench with a downward-oriented trench opening and 1 corresponds to a trench with an upward-oriented trench opening.\n",
    "\n",
    "**vertical_spacing (float)** : Parameter for setting the distance of plots being viewed.\n",
    "\n",
    "Running the following widget will display y-cropped images for each fov and timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3446ed3d691844a7bae1fd4c24854240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='triangle_nbins', max=200, min=10, step=10), FloatSlider…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "y_cropping = interactive(kymo.preview_y_crop,{\"manual\":True},y_percentiles_smoothed_list=fixed(y_percentiles_smoothed_list),\\\n",
    "                imported_array_list=fixed(imported_array_list),\\\n",
    "                triangle_nbins=IntSlider(value=50, min=10, max=200, step=10),\\\n",
    "                triangle_scaling=FloatSlider(value=1.05, min=0., max=2., step=0.05),\\\n",
    "                y_min_edge_dist=IntSlider(value=50, min=10, max=200, step=10),\\\n",
    "                padding_y=IntSlider(value=20, min=0, max=100, step=1),\\\n",
    "                trench_len_y=IntSlider(value=270, min=0, max=1000, step=10),\n",
    "               vertical_spacing=FloatSlider(value=0.9, min=0., max=2., step=0.01),\\\n",
    "                expected_num_rows=IntText(value=2,description='Number of Rows:',disabled=False),\\\n",
    "               orientation_detection=Dropdown(options=[0, 1, 'phase'],value=0,description='Orientation:',disabled=False),\n",
    "                orientation_on_fail=Dropdown(options=[None,0, 1],value=0,description='Orientation when < expected rows:',disabled=False))\n",
    "display(y_cropping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate \"trench-row\" cropping output\n",
    "\n",
    "After determining your desired hyperparameters, set them in the next cell and run it to produce output for later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_in_y_list = copy.copy(y_cropping.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 290, 2048, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_in_y_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune trench detection hyperparameters\n",
    "\n",
    "Next, we will detect the positions of trenchs in the y-cropped images as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the x-axis by computing the qth percentile of the data along the y-axis.\n",
    "2. Determine the signal background by smooth this signal using a large median kernel.\n",
    "3. Subtract the background signal.\n",
    "4. Smooth the resultant signal using a median kernel.\n",
    "5. Use a [otsu threhsold](https://imagej.net/Auto_Threshold#Otsu) to determine the trench midpoint poisitons.\n",
    "\n",
    "This method uses the following `kwargs`, which you can tune here:\n",
    "\n",
    "**x_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    "**background_kernel_x (int)** : Median kernel size to use for step 2.\n",
    "\n",
    "**smoothing_kernel_x (int)** : Median kernel size to use for step 4.\n",
    "\n",
    "**otsu_nbins (int)** : Number of bins to use in the Otsu's method histogram.\n",
    "\n",
    "**otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "**vertical_spacing (float)** : Parameter for setting the distance of plots being viewed.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9574df84dcf4a4d8d92af178f4cbd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='t', max=3), IntSlider(value=85, description='x_percentil…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trench_detection = interactive(kymo.preview_x_percentiles, {\"manual\":True}, cropped_in_y_list=fixed(cropped_in_y_list),t=IntSlider(value=0, min=0, max=cropped_in_y_list[0].shape[4]-1, step=1),\\\n",
    "                x_percentile=IntSlider(value=85, min=50, max=100, step=1),background_kernel_x=IntSlider(value=21, min=1, max=601, step=20), smoothing_kernel_x=IntSlider(value=9, min=1, max=31, step=2),\\\n",
    "               otsu_nbins=IntSlider(value=50, min=10, max=200, step=10),otsu_scaling=FloatSlider(value=0.25, min=0., max=2., step=0.01),\\\n",
    "               vertical_spacing=FloatSlider(value=0.9, min=0., max=2., step=0.01));\n",
    "display(trench_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate trench detection output\n",
    "\n",
    "After determining your desired hyperparameters, set them in the next cell and run it to produce output for later steps. **Note: The thresholding parameters do not need to be specified at this point.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_x_percentiles_list = trench_detection.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check midpoint drift\n",
    "\n",
    "Next, we will perform x-dimension drift correction of our detected midpoints as follows:\n",
    "\n",
    "1. Begin at t=1\n",
    "2. For $m \\in \\{midpoints(t)\\}$ assign $n \\in \\{midpoints(t-1)\\}$ to m if n is the closest midpoint to m at time $t-1$,\n",
    "points that are not the closest midpoint to any midpoints in m will not be mapped.\n",
    "3. Compute the translation of each midpoint at time.\n",
    "4. Take the average of this value as the x-dimension drift from time t-1 to t.\n",
    "\n",
    "This method uses the following `kwargs`, which you can tune here:\n",
    "\n",
    "**otsu_nbins (int)** : Number of bins to use in the Otsu's method histogram.\n",
    "\n",
    "**otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "**vertical_spacing (float)** : Parameter for setting the distance of plots being viewed.\n",
    "\n",
    "Running the following widget will display the detected midpoints for each of your timepoints. If there is too much sparsity, or discontinuity, your drift correction will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb47d70610b489c96b1f3c3ba4d7114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='otsu_nbins', max=200, min=10, step=10), FloatSlider(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "midpoint_drift = interactive(kymo.preview_midpoints,{\"manual\":True},smoothed_x_percentiles_list=fixed(smoothed_x_percentiles_list),\\\n",
    "                otsu_nbins=IntSlider(value=50, min=10, max=200, step=10),\n",
    "                otsu_scaling=FloatSlider(value=1., min=0., max=2., step=0.01),\\\n",
    "               vertical_spacing=FloatSlider(value=0.8, min=0., max=2., step=0.01));\n",
    "display(midpoint_drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate midpoint drift output\n",
    "\n",
    "After determining your desired hyperparameters, set them in the next cell and run it to produce output for later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_midpoints_list,x_drift_list = midpoint_drift.result\n",
    "\n",
    "# otsu_nbins = 50\n",
    "# otsu_scaling = 0.7\n",
    "\n",
    "\n",
    "# all_midpoints_list = kymo.map_to_fovs(kymo.get_all_midpoints,smoothed_x_percentiles_list,otsu_nbins,otsu_scaling)\n",
    "# x_drift_list = kymo.map_to_fovs(kymo.get_x_drift,all_midpoints_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f407e2486439460caf254f2113bb67ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=30, description='trench_width_x', max=50, min=10), FloatSlider(value=0.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corrected_midpoints = interactive(kymo.preview_corrected_midpoints,{\"manual\":True},all_midpoints_list=fixed(all_midpoints_list),\\\n",
    "                                  x_drift_list = fixed(x_drift_list),trench_width_x=IntSlider(value=30, min=10, max=50, step=1),\\\n",
    "                                  trench_present_thr=FloatSlider(value=0., min=0., max=1., step=0.05),\\\n",
    "                                  vertical_spacing=FloatSlider(value=0.8, min=0., max=2., step=0.01));\n",
    "display(corrected_midpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune trench cropping hyperparameters\n",
    "\n",
    "Trench cropping simply uses the drift-corrected midpoints as a reference and crops out some fixed length around them to produce an output kymograph\n",
    "\n",
    "This method uses the following `kwargs`, which you can tune here:\n",
    "\n",
    "**trench_width_x (int)** : Trench width to use for cropping.\n",
    "\n",
    "**vertical_spacing (float)** : Parameter for setting the distance of plots being viewed.\n",
    "\n",
    "Running the following widget will display a random kymograph for each row in each fov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c089b842c304b67b8bc1955908dddab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=30, description='trench_width_x', max=50, min=10), FloatSlider(value=0.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "interact_manual(kymo.preview_kymographs,cropped_in_y_list=fixed(cropped_in_y_list),all_midpoints_list=fixed(all_midpoints_list),\\\n",
    "                x_drift_list=fixed(x_drift_list),trench_width_x=IntSlider(value=30, min=10, max=50, step=1),\\\n",
    "                trench_present_thr=FloatSlider(value=0., min=0., max=1., step=0.05),\\\n",
    "               vertical_spacing=FloatSlider(value=0.8, min=0., max=2., step=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Percentile 88\n",
      "Y Smoothing Kernel 17\n",
      "Triangle Trheshold Bins 50\n",
      "Triangle Threshold Scaling 1.05\n",
      "Minimum Trench Length 50\n",
      "Y Padding 20\n",
      "Trench Length 270\n",
      "Orientation Detection Method 0\n",
      "Expected Number of Rows (Manual Orientation Detection) 2\n",
      "Top Orientation when Row Drifts Out (Manual Orientation Detection) 0\n",
      "X Percentile 85\n",
      "X Background Kernel 21\n",
      "X Smoothing Kernel 9\n",
      "Otsu Trheshold Bins 50\n",
      "Otsu Threshold Scaling 0.3\n",
      "Trench Width 25\n",
      "Trench Presence Threshold 0.7\n"
     ]
    }
   ],
   "source": [
    "kymo.print_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mothermachine",
   "language": "python",
   "name": "mothermachine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
