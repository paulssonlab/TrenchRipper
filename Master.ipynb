{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# TrenchRipper Master Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trenchripper as tr\n",
    "\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, FloatSlider, IntSlider, Dropdown, IntText, SelectMultiple, IntRangeSlider\n",
    "import ipywidgets as widgets\n",
    "import matplotlib\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nd2reader import ND2Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.0\n"
     ]
    }
   ],
   "source": [
    "import nd2reader\n",
    "dir(nd2reader)\n",
    "print(nd2reader.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Paths\n",
    "\n",
    "Begin by defining the directory in which all processing will be done, as well as the initial nd2 file we will be processing."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 2,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath = \"/n/scratch2/bj66/vibrio_analysis_evaluation\"\n",
    "nd2file = \"/n/scratch2/bj66/vibrio_analysis_evaluation/Vibrio_30_MUX002.nd2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "#### Transfer files into the scratch folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sourcedir = \"/n/files/SysBio/PAULSSON\\ LAB/Personal\\ Folders/Leoncini/DATA_Ti4/20190924--Vibrio30_MUX/test\"\n",
    "targetdir = \"/n/scratch2/bj66/vibrio_analysis_evaluation\"\n",
    "tr.cluster.transferjob(sourcedir,targetdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Extract to hdf5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bj66/anaconda3/lib/python3.7/site-packages/bokeh/plotting/helpers.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, OrderedDict, Sequence\n"
     ]
    }
   ],
   "source": [
    "dask_controller = tr.cluster.dask_controller(walltime='04:00:00',local=False,n_workers=40,memory='2GB')\n",
    "dask_controller.startdask()\n",
    "dask_controller.daskcluster.start_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://10.120.16.117:8787/status\">Dashboard</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = tr.ndextract.hdf5_fov_extractor(nd2file,headpath,tpts_per_file=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1ecdd1dc2547028227e2e47e97746c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='', description='Organism:', placeholder='Organism imaged in this experim…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hdf5_extractor.inter_get_notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bj66/anaconda3/lib/python3.7/site-packages/nd2reader/common_raw_metadata.py:94: RuntimeWarning: Reported average frame interval (56018.0 ms) doesn't match the set interval (60000.0 ms). Using the average now.\n",
      "  warnings.warn(message % (avg_interval, interval), RuntimeWarning)\n",
      "/home/bj66/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'height': 1788, 'width': 2048, 'date': datetime.datetime(2019, 9, 24, 17, 13, 21), 'fields_of_view': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], 'frames': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'z_levels': [], 'total_images_per_channel': 1110, 'channels': ['Phase'], 'pixel_microns': 0.065, 'num_frames': 15, 'experiment': {'description': 'Vibrio_30_MUX', 'loops': [{'start': 0, 'duration': -1.0, 'stimulation': False, 'sampling_interval': 56017.95771428517}]}, 'num_fovs': 74, 'settings': {'Phase': {'camera_name': 'Andor Zyla VSC-04365', 'obj_settings': {'wsObjectiveName': 'Plan Fluor 100x Oil Ph3 ADH', 'dObjectiveMag': 100.0, 'dObjectiveNA': 1.3, 'dRefractIndex': 1.515}, 'Camera_Type': 'Andor_Zyla', 'Binning': '1x1', 'Exposure': '50_ms', 'Readout_Mode': 'Rolling_shutter_at_16-bit', 'Readout_Rate': '200_MHz_', 'Conversion_Gain': 'Dual_Gain_1/4', 'Spurious_Noise_Filter': 'on', 'Sensor_Mode': 'Normal', 'Trigger_Mode': 'Internal', 'Temperature': '-0.4°C'}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bj66/anaconda3/lib/python3.7/site-packages/tables/path.py:118: NaturalNameWarning: object name is a Python keyword: 'global'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  % (name, _warnInfo), NaturalNameWarning)\n",
      "/home/bj66/TrenchRipper/trenchripper/utils.py:91: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  df.metadata = store.get_storer(key).attrs.metadata\n"
     ]
    }
   ],
   "source": [
    "hdf5_extractor.extract(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shutdown Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Worker tcp://10.120.16.179:60960 restart in Job 52352597. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.179:56984 restart in Job 52352596. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.179:42002 restart in Job 52352554. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.12:46018 restart in Job 52352564. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.12:45818 restart in Job 52352565. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.12:55851 restart in Job 52352563. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.181:37246 restart in Job 52352589. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.180:36471 restart in Job 52352591. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.180:48799 restart in Job 52352592. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.180:51283 restart in Job 52352556. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.180:56880 restart in Job 52352555. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.180:50531 restart in Job 52352595. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.180:52372 restart in Job 52352590. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.180:37324 restart in Job 52352557. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.180:40853 restart in Job 52352593. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.180:52514 restart in Job 52352594. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.141:60345 restart in Job 52352569. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.107:43827 restart in Job 52352567. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.146:43323 restart in Job 52352568. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.53:52950 restart in Job 52352572. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.118:42658 restart in Job 52352573. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.187:50475 restart in Job 52352559. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.97:58694 restart in Job 52352583. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.60:52178 restart in Job 52352585. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.184:40236 restart in Job 52352588. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.49:40524 restart in Job 52352582. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.185:58035 restart in Job 52352558. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.185:57339 restart in Job 52352576. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.191:55760 restart in Job 52352579. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.191:41618 restart in Job 52352580. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.191:56947 restart in Job 52352578. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.13:41275 restart in Job 52352561. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.13:38158 restart in Job 52352560. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.115:60245 restart in Job 52352584. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.41:55451 restart in Job 52352581. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.79:49637 restart in Job 52352566. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.65:48892 restart in Job 52352587. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.124:55289 restart in Job 52352574. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.124:57575 restart in Job 52352575. This can be due to memory issue.\n",
      "distributed.scheduler - ERROR - Not all workers responded positively: ['OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'timed out', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK', 'OK']\n",
      "NoneType: None\n",
      "Worker tcp://10.120.16.94:43854 restart in Job 52352570. This can be due to memory issue.\n"
     ]
    }
   ],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Kymographs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Test Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "#### Initialize the interactive kymograph class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.kymograph_interactive` class that will be handling all steps of generating a kymograph. \n",
    "\n",
    "You will need to specify the following `args` and `kwargs` (in order):\n",
    "\n",
    "\n",
    "**Args**\n",
    "\n",
    "**input_file_prefix (string)** : File prefix for all input hdf5 files of the form \"\\[input_file_prefix\\]\\[number\\].hdf5\" This should be the default output format for the hdf5 export code, but you will need to rename files if taking input files from a different source.\n",
    "\n",
    "**all_channels (list)** : list of strings corresponding to the different image channels available in the input hdf5 file, with the channel used for segmenting trenches in the first position. NOTE: these names must match those of the input hdf5 file datasets.\n",
    "\n",
    "**fov_list (list)** : List of ints corresponding to the fovs that you wish to make kymographs of.\n",
    "\n",
    "**Kwargs**\n",
    "\n",
    "**t_subsample_step (int)** : Step size to be used for subsampling input files in time, recommend that subsampling results in between 5 and 20 timepoints for quick processing.\n",
    "\n",
    "**t_range (tuple of ints)** : Range size to be used for subsampling input files in time.\n",
    "\n",
    "The last line will perform import and subsampling of the input hdf5 image files."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 3,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/home/bj66/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/bj66/TrenchRipper/trenchripper/utils.py:91: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
=======
      "/home/de64/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/de64/TrenchRipper/trenchripper/utils.py:91: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
>>>>>>> upstream/master
      "  df.metadata = store.get_storer(key).attrs.metadata\n"
     ]
    }
   ],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "interactive_kymograph = tr.interactive.kymograph_interactive(headpath)\n",
    "channels,fov_list,timepoints_len = interactive_kymograph.get_image_params()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 4,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "c18da2945a0146918f882c337ca073e1",
=======
       "model_id": "3d7269110d464c258aa01ff84c385a45",
>>>>>>> upstream/master
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntText(value=0, description='FOV number:'), IntSlider(value=0, continuous_update=False,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(interactive_kymograph.view_image,fov_idx=IntText(value=0,description='FOV number:',disabled=False),\\\n",
    "         t=IntSlider(value=0, min=0, max=timepoints_len-1, step=1,continuous_update=False),\n",
    "        channel=Dropdown(options=channels,value=channels[0],description='Channel:',disabled=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Could this be made more robust? E.g. edge detection </i>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 5,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "58de0f452cce4cb99eac50062beaa9c3",
=======
       "model_id": "58df52f879f84480aadc36f5cff543dc",
>>>>>>> upstream/master
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
<<<<<<< HEAD
       "interactive(children=(Dropdown(description='seg_channel', options=('Phase',), value='Phase'), SelectMultiple(d…"
=======
       "interactive(children=(Dropdown(description='seg_channel', options=('Phase', 'GFP'), value='Phase'), SelectMult…"
>>>>>>> upstream/master
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import_hdf5 = interactive(interactive_kymograph.import_hdf5_files, {\"manual\":True},all_channels=fixed(channels),seg_channel=Dropdown(options=channels,value=channels[0]),\\\n",
    "                          fov_list=SelectMultiple(options=fov_list),t_range=IntRangeSlider(value=[0, timepoints_len-1],min=0,max=timepoints_len-1,step=1,disabled=False,continuous_update=False),\\\n",
    "                          t_subsample_step=IntSlider(value=10, min=0, max=200, step=1));\n",
    "display(import_hdf5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 6,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_array_list = copy.copy(import_hdf5.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune \"trench-row\" detection hyperparameters\n",
    "\n",
    "The kymograph code begins by detecting the positions of trench rows in the image as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the y-axis by computing the qth percentile of the data along the x-axis\n",
    "2. Smooth this signal using a median kernel\n",
    "3. Use a [triangle threshold](https://imagej.net/Auto_Threshold#Triangle) to determine the trench row poisitons\n",
    "\n",
    "This method uses the following `kwargs`, which you can tune here:\n",
    "\n",
    "**y_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    "**smoothing_kernel_y_dim_0 (int)** : Median kernel size to use for step 2.\n",
    "\n",
    "**triangle_nbins (int)** : Number of bins to use in the triangle method histogram.\n",
    "\n",
    "**triangle_scaling (float)** : Scaling factor to apply to the threshold determined by the triangle method.\n",
    "\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 7,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "0f6546a048ba41e593fd4c3b32b25942",
=======
       "model_id": "07912fc18ccc468a997860378df9b5a2",
>>>>>>> upstream/master
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='y_percentile'), IntSlider(value=17, description='smoot…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "row_detection = interactive(interactive_kymograph.preview_y_precentiles, {\"manual\":True},imported_array_list=fixed(imported_array_list),y_percentile=IntSlider(value=100, min=0, max=100, step=1),\\\n",
    "         smoothing_kernel_y_dim_0=IntSlider(value=17, min=1, max=200, step=2),triangle_nbins=IntSlider(value=50, min=10, max=300, step=10),\\\n",
    "                triangle_scaling=FloatSlider(value=3.5, min=0., max=4., step=0.05),\\\n",
    "            triangle_threshold_bounds=IntRangeSlider(value=[0, 5000],min=0,max=5000,step=50,disabled=False,continuous_update=False))\n",
    "display(row_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate \"trench-row\" detection output\n",
    "\n",
    "After determining your desired hyperparameters, set them in the next cell and run it to produce output for later steps. **Note: The thresholding parameters do not need to be specified at this point.**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 8,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [],
   "source": [
    "y_percentiles_smoothed_list = copy.copy(row_detection.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1788, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_percentiles_smoothed_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune \"trench-row\" cropping hyperparameters\n",
    "\n",
    "Next, we will use the detected rows to perform cropping of the input image in the y-dimension:\n",
    "\n",
    "1. Determine edges of trench rows based on threshold mask.\n",
    "2. Filter out rows that are too small.\n",
    "3. Perform cropping using the \"end\" of the row as reference (the end referring to the part of the trench farthest from the feeding channel).\n",
    "\n",
    "This method uses the following `kwargs`, which you can tune here:\n",
    "\n",
    "**y_min_edge_dist (int)** : Minimum row length necessary for detection.\n",
    "\n",
    "**padding_y (int)** : Padding to be used when cropping in the y-dimension.\n",
    "\n",
    "**trench_len_y (int)** : Length from the end of the tenches to be used when cropping in the y-dimension. <i> why can't this just be found using the edge nearest to the feeding channel? </i>\n",
    "\n",
    "**top_orientation (int)** : The orientation of the top-most row where 0 corresponds to a trench with a downward-oriented trench opening and 1 corresponds to a trench with an upward-oriented trench opening.\n",
    "\n",
    "**vertical_spacing (float)** : Parameter for setting the distance of plots being viewed.\n",
    "\n",
    "Running the following widget will display y-cropped images for each fov and timepoint."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 9,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "88566731270e4a65862b597fe3668d92",
=======
       "model_id": "89a921897d114150b766110353c02e9c",
>>>>>>> upstream/master
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='y_min_edge_dist', max=200, min=10, step=10), IntSlider(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "y_cropping = interactive(interactive_kymograph.preview_y_crop,{\"manual\":True},y_percentiles_smoothed_list=fixed(y_percentiles_smoothed_list),\\\n",
    "                imported_array_list=fixed(imported_array_list),\\\n",
    "                y_min_edge_dist=IntSlider(value=50, min=10, max=200, step=10),\\\n",
    "                padding_y=IntSlider(value=20, min=0, max=100, step=1),\\\n",
    "                trench_len_y=IntText(value=270, description=\"Trench Length (y)\"),\n",
    "               vertical_spacing=FloatSlider(value=0.9, min=0., max=2., step=0.01),\\\n",
    "                expected_num_rows=IntText(value=2,description='Number of Rows:',disabled=False),\\\n",
    "               orientation_detection=Dropdown(options=[0, 1, 'phase'],value=0,description='Orientation:',disabled=False),\n",
    "                orientation_on_fail=Dropdown(options=[None,0, 1],value=0,description='Orientation when < expected rows:',disabled=False))\n",
    "display(y_cropping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate \"trench-row\" cropping output\n",
    "\n",
    "After determining your desired hyperparameters, set them in the next cell and run it to produce output for later steps."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 10,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_in_y_list = copy.copy(y_cropping.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune trench detection hyperparameters\n",
    "\n",
    "Next, we will detect the positions of trenchs in the y-cropped images as follows:\n",
    "\n",
    "1. Reducing each 2D image to a 1D signal along the x-axis by computing the qth percentile of the data along the y-axis.\n",
    "2. Determine the signal background by smooth this signal using a large median kernel.\n",
    "3. Subtract the background signal.\n",
    "4. Smooth the resultant signal using a median kernel.\n",
    "5. Use a [otsu threhsold](https://imagej.net/Auto_Threshold#Otsu) to determine the trench midpoint poisitons.\n",
    "\n",
    "This method uses the following `kwargs`, which you can tune here:\n",
    "\n",
    "**x_percentile (int)** : Percentile to use for step 1.\n",
    "\n",
    "**background_kernel_x (int)** : Median kernel size to use for step 2.\n",
    "\n",
    "**smoothing_kernel_x (int)** : Median kernel size to use for step 4.\n",
    "\n",
    "**otsu_nbins (int)** : Number of bins to use in the Otsu's method histogram.\n",
    "\n",
    "**otsu_scaling (float)** : Scaling factor to apply to the threshold determined by Otsu's method.\n",
    "\n",
    "**vertical_spacing (float)** : Parameter for setting the distance of plots being viewed.\n",
    "\n",
    "Running the following widget will display the smoothed 1-D signal for each of your timepoints. In addition, the threshold value for each fov will be displayed as a red line."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 11,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "0bf926d88e9444e3ac8d1b6b76710edd",
=======
       "model_id": "7212ded70e664f538b856bd1e17bd678",
>>>>>>> upstream/master
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
<<<<<<< HEAD
       "interactive(children=(IntSlider(value=0, description='t', max=1), IntSlider(value=85, description='x_percentil…"
=======
       "interactive(children=(IntSlider(value=0, description='t', max=4), IntSlider(value=85, description='x_percentil…"
>>>>>>> upstream/master
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trench_detection = interactive(interactive_kymograph.preview_x_percentiles, {\"manual\":True}, cropped_in_y_list=fixed(cropped_in_y_list),t=IntSlider(value=0, min=0, max=cropped_in_y_list[0].shape[4]-1, step=1),\\\n",
    "                x_percentile=IntSlider(value=85, min=50, max=100, step=1),background_kernel_x=IntSlider(value=21, min=1, max=601, step=20), smoothing_kernel_x=IntSlider(value=9, min=1, max=31, step=2),\\\n",
    "               otsu_nbins=IntSlider(value=50, min=10, max=200, step=10),otsu_scaling=FloatSlider(value=0.25, min=0., max=2., step=0.01),\\\n",
    "               vertical_spacing=FloatSlider(value=0.9, min=0., max=2., step=0.01));\n",
    "display(trench_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate trench detection output\n",
    "\n",
    "After determining your desired hyperparameters, set them in the next cell and run it to produce output for later steps. **Note: The thresholding parameters do not need to be specified at this point.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_x_percentiles_list = trench_detection.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check midpoint drift\n",
    "\n",
    "Next, we will perform x-dimension drift correction of our detected midpoints as follows:\n",
    "\n",
    "1. Begin at t=1\n",
    "2. For $m \\in \\{midpoints(t)\\}$ assign $n \\in \\{midpoints(t-1)\\}$ to m if n is the closest midpoint to m at time $t-1$,\n",
    "points that are not the closest midpoint to any midpoints in m will not be mapped.\n",
    "3. Compute the translation of each midpoint at time.\n",
    "4. Take the average of this value as the x-dimension drift from time t-1 to t.\n",
    "\n",
    "This method uses the following `kwargs`, which you can tune here:\n",
    "\n",
    "**vertical_spacing (float)** : Parameter for setting the distance of plots being viewed.\n",
    "\n",
    "Running the following widget will display the detected midpoints for each of your timepoints. If there is too much sparsity, or discontinuity, your drift correction will not be accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Number/trench interest point registration (SIFT)? Autocorrelation? Single-dimension Euclidean distances between intensity maps of trench-sized boxes?</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbd8bee892b480d975e7374d5d47eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.8, description='vertical_spacing', max=2.0, step=0.01), Button(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "midpoint_drift = interactive(interactive_kymograph.preview_midpoints,{\"manual\":True},smoothed_x_percentiles_list=fixed(smoothed_x_percentiles_list),\\\n",
    "               vertical_spacing=FloatSlider(value=0.8, min=0., max=2., step=0.01));\n",
    "display(midpoint_drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate midpoint drift output\n",
    "\n",
    "After determining your desired hyperparameters, set them in the next cell and run it to produce output for later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_midpoints_list,x_drift_list = midpoint_drift.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune trench cropping hyperparameters\n",
    "\n",
    "Trench cropping simply uses the drift-corrected midpoints as a reference and crops out some fixed length around them to produce an output kymograph\n",
    "\n",
    "This method uses the following `kwargs`, which you can tune here:\n",
    "\n",
    "**trench_width_x (int)** : Trench width to use for cropping.\n",
    "\n",
    "**vertical_spacing (float)** : Parameter for setting the distance of plots being viewed.\n",
    "\n",
    "Running the following widget will display a random kymograph for each row in each fov.\n",
    "\n",
    "It will also produce midpoint plots showing retained midpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e42195ecfcf44088c4fe07c0ddc9ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=30, description='trench_width_x', min=10, step=2), FloatSlider(value=0.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "interact_manual(interactive_kymograph.preview_kymographs,cropped_in_y_list=fixed(cropped_in_y_list),all_midpoints_list=fixed(all_midpoints_list),\\\n",
    "                x_drift_list=fixed(x_drift_list),trench_width_x=IntSlider(value=30, min=10, max=100, step=2),\\\n",
    "                trench_present_thr=FloatSlider(value=0., min=0., max=1., step=0.05),\\\n",
    "               vertical_spacing=FloatSlider(value=0.8, min=0., max=2., step=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Why do we store data as kymographs? </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export and save hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Percentile 80\n",
      "Y Smoothing Kernel 17\n",
      "Triangle Threshold Bins 90\n",
      "Triangle Threshold Scaling 1.35\n",
      "Minimum Trench Length 50\n",
      "Y Padding 20\n",
      "Trench Length 700\n",
      "Orientation Detection Method 0\n",
      "Expected Number of Rows (Manual Orientation Detection) 1\n",
      "Top Orientation when Row Drifts Out (Manual Orientation Detection) 0\n",
      "X Percentile 85\n",
      "X Background Kernel 221\n",
      "X Smoothing Kernel 17\n",
      "Otsu Threshold Bins 50\n",
      "Otsu Threshold Scaling 0.31\n",
      "Trench Width 50\n",
      "Trench Presence Threshold 0.0\n",
      "All Channels ['Phase']\n",
      "Time Range (0, 15)\n"
     ]
    }
   ],
   "source": [
    "interactive_kymograph.process_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_kymograph.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Generate Kymograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask Workers"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 3,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/home/bj66/anaconda3/lib/python3.7/site-packages/bokeh/plotting/helpers.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, OrderedDict, Sequence\n"
=======
      "/home/de64/anaconda3/lib/python3.7/site-packages/bokeh/plotting/helpers.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, OrderedDict, Sequence\n",
      "/home/de64/anaconda3/lib/python3.7/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n",
      "/home/de64/anaconda3/lib/python3.7/site-packages/distributed/dashboard/core.py:75: ResourceWarning: unclosed <socket.socket fd=55, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::', 0, 0, 0)>\n",
      "  raise\n"
>>>>>>> upstream/master
     ]
    }
   ],
   "source": [
    "dask_controller = tr.cluster.dask_controller(walltime='04:00:00',local=False,n_workers=20,memory='4GB')\n",
    "dask_controller.startdask()\n",
    "dask_controller.daskcluster.start_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
<<<<<<< HEAD
       "<a href=\"http://10.120.17.9:8787/status\">Dashboard</a>"
=======
       "<a href=\"http://10.120.16.179:47571/status\">Dashboard</a>"
>>>>>>> upstream/master
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 5,
>>>>>>> upstream/master
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Triangle Max Threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-70125a57808a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkymoclust\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkymograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkymograph_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheadpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrenches_per_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparamfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/TrenchRipper/trenchripper/kymograph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, headpath, trenches_per_file, paramfile, all_channels, trench_len_y, padding_y, trench_width_x, t_range, y_percentile, y_min_edge_dist, smoothing_kernel_y, triangle_nbins, triangle_scaling, triangle_max_threshold, triangle_min_threshold, top_orientation, expected_num_rows, orientation_on_fail, x_percentile, background_kernel_x, smoothing_kernel_x, otsu_nbins, otsu_scaling, trench_present_thr)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtriangle_nbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Triangle Threshold Bins\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtriangle_scaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Triangle Threshold Scaling\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mtriangle_max_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Triangle Max Threshold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mtriangle_min_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Triangle Min Threshold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mtop_orientation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Orientation Detection Method\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Triangle Max Threshold'"
     ]
    }
   ],
   "source": [
    "kymoclust = tr.kymograph.kymograph_cluster(headpath=headpath,trenches_per_file=20,paramfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bj66/TrenchRipper/trenchripper/utils.py:91: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  df.metadata = store.get_storer(key).attrs.metadata\n",
      "/home/bj66/anaconda3/lib/python3.7/site-packages/bokeh/core/property/container.py:103: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(value, (collections.Container, collections.Sized, collections.Iterable))\n"
     ]
    }
   ],
   "source": [
    "kymoclust.generate_kymographs(dask_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.post_process(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check kymograph statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymoclust.kymo_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shutdown Dask"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Worker tcp://10.120.16.192:49717 restart in Job 52359202. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.13:50000 restart in Job 52359189. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.192:53878 restart in Job 52359199. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.13:43041 restart in Job 52359191. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.192:36391 restart in Job 52359201. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.13:38638 restart in Job 52359190. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.192:47320 restart in Job 52359200. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.191:60985 restart in Job 52359207. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.191:50421 restart in Job 52359208. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.191:56622 restart in Job 52359205. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.191:56936 restart in Job 52359206. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.191:51623 restart in Job 52359203. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.14:46326 restart in Job 52359193. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.14:55394 restart in Job 52359195. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.14:56324 restart in Job 52359192. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.14:58955 restart in Job 52359197. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.14:53826 restart in Job 52359196. This can be due to memory issue.\n",
      "Worker tcp://10.120.17.16:43425 restart in Job 52359188. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.193:60197 restart in Job 52359198. This can be due to memory issue.\n",
      "Worker tcp://10.120.16.191:53784 restart in Job 52359187. This can be due to memory issue.\n"
     ]
    }
   ],
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
>>>>>>> upstream/master
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Fluorescence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Test Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the interactive segmentation class\n",
    "\n",
    "As a first step, initialize the `tr.interactive.fluo_segmentation_interactive` class that will be handling all steps of generating a segmentation. \n",
    "\n",
    "You will need to specify the following `args` (in order):\n",
    "\n",
    "\n",
    "**Args**\n",
    "\n",
    "**headpath (string)** : Top level path being used for processing (same as the rest of the mother machine pipeline.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "interactive_segmentation = tr.interactive.fluo_segmentation_interactive(headpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose channel to segment on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_channel = interactive(interactive_segmentation.choose_seg_channel, {\"manual\":True},seg_channel=Dropdown(options=interactive_segmentation.all_channels,\\\n",
    "                                                                                              value=interactive_segmentation.all_channels[0]));\n",
    "display(choose_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data\n",
    "\n",
    "Fill in \n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**fov_idx (int)** :\n",
    "\n",
    "**n_trenches (int)** :\n",
    "\n",
    "**t_range (tuple)** :\n",
    "\n",
    "**t_subsample_step (int)** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_arr_int = interactive(interactive_segmentation.import_array, {\"manual\":True},\\\n",
    "                       n_trenches=IntText(value=1,description='Number of trenches:',disabled=False),\\\n",
    "                       t_range=IntRangeSlider(value=[interactive_segmentation.t_range[0],interactive_segmentation.t_range[1]-1],\\\n",
    "                        description='Time Range:',min=interactive_segmentation.t_range[0],max=interactive_segmentation.t_range[1]-1,step=1,disabled=False),\\\n",
    "                       t_subsample_step=IntSlider(value=1,description='Time Subsampling Step:', min=1, max=20, step=1),\\\n",
    "                       fig_size_y=IntSlider(value=9, description='Figure Size (Y Dimension):' , min=1, max=30, step=1),\\\n",
    "                       fig_size_x=IntSlider(value=6, description='Figure Size (X Dimension):', min=1, max=30, step=1),\\\n",
    "                          img_per_row=IntSlider(value=2, description='Images per Row:', min=1, max=30, step=1));\n",
    "display(kymo_arr_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kymo_arr = copy.copy(kymo_arr_int.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale data\n",
    "\n",
    "Fill in \n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**scale (bool)** : Whether to scale the kymograph in time.\n",
    "\n",
    "**scaling_percentile (int)** : Whole image intensity percentile to use to determine scaling constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_list_int = interactive(interactive_segmentation.plot_scaled, {\"manual\":True},kymo_arr=fixed(kymo_arr),\\\n",
    "                          scale=Dropdown(options=[True,False],value=True,description='Scale Fluorescence?',disabled=False),\\\n",
    "                          scaling_percentile=IntSlider(value=90,description='Scaling Percentile:',min=0,max=100,step=1,disabled=False));\n",
    "display(scaled_list_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_list = copy.copy(scaled_list_int.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Gaussian Filter\n",
    "\n",
    "Fill in \n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**smooth_sigma (float)** : Standard deviation of gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_list_int = interactive(interactive_segmentation.plot_processed, {\"manual\":True},scaled_list=fixed(scaled_list),\\\n",
    "                          smooth_sigma=FloatSlider(value=0.75,description='Gaussian Kernel Sigma:',min=0.,max=3.,step=0.25,disabled=False));\n",
    "display(proc_list_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_list = copy.copy(proc_list_int.result)\n",
    "eigval_list = interactive_segmentation.plot_eigval(proc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(eigval_list[3]>125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Cell Mask Envelope\n",
    "\n",
    "Fill in.\n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**cell_mask_method (str)** : Thresholding method, can be a local or global Otsu threshold.\n",
    "\n",
    "**cell_otsu_scaling (float)** : Scaling factor applied to determined threshold.\n",
    "\n",
    "**local_otsu_r (int)** : Radius of thresholding kernel used in the local otsu thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_mask_list_int = interactive(interactive_segmentation.plot_cell_mask, {\"manual\":True},proc_list=fixed(proc_list),\\\n",
    "                          cell_mask_method=Dropdown(options=['local','global'],value='local',description='Cell Mask Thresholding Method:',disabled=False),\\\n",
    "                          global_otsu_scaling=FloatSlider(value=0.95,description='Global Threshold Scaling:',min=0.,max=2.,step=0.01,disabled=False),\\\n",
    "                          cell_otsu_scaling=FloatSlider(value=0.95,description='Cell Threshold Scaling:',min=0.,max=2.,step=0.01,disabled=False),\\\n",
    "                          local_otsu_r=IntSlider(value=15,description='Local Otsu Radius:',min=0,max=30,step=1,disabled=False));\n",
    "display(cell_mask_list_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_mask_list = copy.copy(cell_mask_list_int.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Edge Mask at Threshold Value\n",
    "\n",
    "Fill in.\n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**edge_threshold_scaling (float)** : Scaling factor applied to determined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_mask_list_int = interactive(interactive_segmentation.plot_threshold_result, {\"manual\":True},eigval_list=fixed(eigval_list),cell_mask_list=fixed(cell_mask_list),\\\n",
    "                          edge_threshold_scaling=FloatSlider(value=1.,description='Edge Threshold Scaling',min=0.,max=2.,step=0.01,disabled=False),\\\n",
    "                          min_obj_size=IntSlider(value=30,description='Minimum Object Size:',min=0,max=100,step=2,disabled=False));\n",
    "display(composite_mask_list_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold Sampling and Convexity Calculation\n",
    "\n",
    "Fill in.\n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**edge_threshold_scaling (float)** : Scaling factor applied to determined threshold.\n",
    "\n",
    "**threshold_step_perc (float)** : Threshold step size to be used for trying multiple thresholds.\n",
    "\n",
    "**threshold_perc_num_steps (int)** : Number of steps to use when generating multiple thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_scores_list_int = interactive(interactive_segmentation.plot_scores, {\"manual\":True},eigval_list=fixed(eigval_list),cell_mask_list=fixed(cell_mask_list),\\\n",
    "                          edge_threshold_scaling=FloatSlider(value=0.9,description='Edge Threshold Scaling',min=0.,max=2.,step=0.01,disabled=False),\\\n",
    "                          threshold_step_perc=FloatSlider(value=0.05,description='Threshold Step Percent',min=0.,max=0.5,step=0.01,disabled=False),\\\n",
    "                          threshold_perc_num_steps=IntSlider(value=2,description='Number of Threshold Steps',min=0,max=5,step=1,disabled=False),\\\n",
    "                          min_obj_size=IntSlider(value=30,description='Minimum Object Size:',min=0,max=100,step=2,disabled=False));\n",
    "display(conv_scores_list_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_scores_list = copy.copy(conv_scores_list_int.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convexity Thresholding\n",
    "\n",
    "Fill in.\n",
    "\n",
    "You will need to tune the following `args` and `kwargs` (in order):\n",
    "\n",
    "**convex_threshold (float)** : Threshold to be used for convexity thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask_list_int = interactive(interactive_segmentation.plot_final_mask, {\"manual\":True},conv_scores_list=fixed(conv_scores_list),\\\n",
    "                          convex_threshold=FloatSlider(value=0.75,description='Convexity Threshold:',min=0.,max=1.,step=0.01,disabled=False));\n",
    "display(final_mask_list_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.process_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_segmentation.write_param_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Generate Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.cluster.dask_controller(walltime='01:00:00',local=False,n_workers=100,memory='500MB')\n",
    "dask_controller.startdask()\n",
    "dask_controller.daskcluster.start_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = tr.segment.fluo_segmentation_cluster(headpath,paramfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment.dask_segment(dask_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Phase Segmentation Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = tr.unet.UNet_Training_DataLoader(nndatapath=\"/n/scratch2/de64/nntest5\",experimentname=\"First NN\",trainpath=\"/n/scratch2/de64/2019-06-18_DE85_training_data\",\\\n",
    "                                      testpath=\"/n/scratch2/de64/2019-05-31_validation_data\",\\\n",
    "                                      valpath=\"/n/scratch2/de64/2019-05-31_validation_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.inter_get_selection(dataloader.trainpath,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.inter_get_selection(dataloader.testpath,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.inter_get_selection(dataloader.valpath,\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weightmap Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.display_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.get_grid_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.export_all_data(memory=\"6GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Hyperparameter (Grid) Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-up Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = tr.unet.GridSearch(\"/n/scratch2/de64/nntest4\",numepochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.display_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.get_grid_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.run_grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.retry_failed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.retry_processing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
